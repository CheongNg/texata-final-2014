{
 "metadata": {
  "name": "",
  "signature": "sha256:c59fd7ce09f6e9fa7da96ca5dfce5917928e139c5fdd9d3553019658e952360b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Cisco Case Study"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "1. Problem statement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case study we are provided with the transcripts of question & answer posts from the Cisco [community support forums](https://supportforums.cisco.com/). \n",
      "Three topics are offered for consideration: *content indexing*, *anomaly detection* and *influencer identification*. Although some explanation of the topics are given, in general, the problem formulation seems to imply that Cisco **is interested in making *any* valuable use of the support forum data**.\n",
      "\n",
      "Such a vaguely phrased problem could be approached in a multitude of ways, depending on the assumptions regarding costs and benefits, needs and limitations. Consequently, I shall start by converting the vague problem statement into a subjectively chosen list of very specific tasks to be tackled during the contest."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "2. Solution approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having thought about the problem statement and having taken a look at the available data, I have settled on the following prioritized list of potential tasks for the contest:\n",
      "\n",
      " 1. Implement a simple duplicate detection and \"related discussions\" functionality, aimed at the users who come to *ask* or *browse* questions.\n",
      " 2. Implement a simple automated tag recommendation system for discussions.\n",
      " 3. Verify the assumption about there being two types of users present in the data, and if so, categorize the currently active users.\n",
      " 4. Attempt to verify there are two major types of discussions (\"assistance\" and \"complaints\") present in the data, and if so, categorize the discussions.\n",
      " 5. Study the relationship between the users and discussions in StackOverflow the Cisco forums.\n",
      " \n",
      "To keep this notebook short, the rationale behind those choices is verbosely described [in a separate notebook](2 - Rationale.ipynb)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3 Solution transcript"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3.0 Getting the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We shall import the data from text files into a database, and use an SQLAlchemy-based ORM abstraction layer on top. The data model and the import procedures are in the package `tx.db`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tx\n",
      "db_url = 'postgresql://texata:__austin__@localhost:5432/texata_rs'\n",
      "data_dir = r'data\\final-hackathon\\RS\\content'\n",
      "tx.db.connect_db(db_url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from tx.db import *\n",
      "init_db()\n",
      "import_dir(data_dir, DBSession, max_files=10000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Traceback (most recent call last):\n",
        "  File \"tx\\db.py\", line 120, in parse_post\n",
        "    assert firstline.strip() == 'Title:'\n",
        "AssertionError\n",
        "ERROR:tx.db:Error parsing file data\\final-hackathon\\RS\\content\\10527706_html.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR:tx.db:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "tx\\db.py:158: UserWarning: Failed to import data\\final-hackathon\\RS\\content\\10527706_html.txt\n",
        "  warnings.warn(\"Failed to import %s\" % f)\n",
        "Traceback (most recent call last):\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  File \"tx\\db.py\", line 120, in parse_post\n",
        "    assert firstline.strip() == 'Title:'\n",
        "AssertionError\n",
        "ERROR:tx.db:Error parsing file data\\final-hackathon\\RS\\content\\10556241_html.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR:tx.db:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "tx\\db.py:158: UserWarning: Failed to import data\\final-hackathon\\RS\\content\\10556241_html.txt\n",
        "  warnings.warn(\"Failed to import %s\" % f)\n",
        "Traceback (most recent call last):\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  File \"tx\\db.py\", line 120, in parse_post\n",
        "    assert firstline.strip() == 'Title:'\n",
        "AssertionError\n",
        "ERROR:tx.db:Error parsing file data\\final-hackathon\\RS\\content\\10688256_html.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR:tx.db:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 3min 3s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "tx\\db.py:158: UserWarning: Failed to import data\\final-hackathon\\RS\\content\\10688256_html.txt\n",
        "  warnings.warn(\"Failed to import %s\" % f)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Above is an example of loading just the first 10k entries. We run [a separate notebook](3 - Loading data.ipynb) to load the full datasets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# At first I used the 10k sample for development and testing.\n",
      "# When the complete data import to the database completed, I reevaluated this and the following cells\n",
      "from tx.db import *\n",
      "ps = DBSession.query(Post).all()\n",
      "print \"Posts: \", len(ps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Posts:  86494\n",
        "Wall time: 20.4 s\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3.1 Automated duplicate detection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several approaches for automatically detecting related or similar texts in the database. Sidestepping the complicated ones, my favourites are the following:\n",
      "\n",
      " - *Text Winnowing* (we did a plagiarism detection system based on that once and it worked nicely), \n",
      " - *Simhash* (great for simplicity and speed for finding near-duplicates), and \n",
      " - *Full text search* (great because a prototype could be implemented by just indexing an SQL database or any other off-the-shelf system).\n",
      "\n",
      "The choice of technology here is not trivial, though, as it requires some experimentation and a subjective decision as to which system provides better suggestions. However, given that Cisco currently has *no* related discussion functionality, anything would be an improvement. In the following I shall try to briefly examine how the three approaches work, in addition getting a feeling for whether there are duplicates in the data. If any of the approaches seems reasonable, it is an immediate suggestion for improvement."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3.1.1 Full text search"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we have the data in the database anyway, adding a full text index is trivial using standard Postgres facilities:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "Engine.execute(\"create index post_tsidx on post using gin(to_tsvector('english', coalesce(title,'') || ' ' || coalesce(body,'')));\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 1min 53s\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<sqlalchemy.engine.result.ResultProxy at 0x1b372a90>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The fulltext-based related posts algorithm can now be the following: we first define a list of \"informative\" words, extract those from the post body and title, and use them in a standard fulltext search query. \n",
      "\n",
      "\"Informative\" words can be defined in various ways. One popular approach is to use a predefined technical lexicon. I do not have one available, though, so here's my house recipe for today:\n",
      "\n",
      " - Consider all words that are present in the data in at least 3 posts.\n",
      " - Subtract from those a list of \"usual and boring\" words (Oxford 3000 is potential list of such words), and the set of stopwords (I'll use the one from NLTK).\n",
      "\n",
      "This wordlist creation procedure was run [in a separate notebook](4 - Informative words.ipynb). The method which uses this wordlist to extract queries is implemented in `tx.features`. The algorithm that uses this method is implemented in `tx.similarity.FulltextDBSearcher`. Some hacking and experimentation went into it, but below is an example of how it performs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "fe = tx.features.compose(tx.features.RandomSummary(), tx.features.SetOfWords())\n",
      "searcher = tx.similarity.FulltextDBSearcher(fe, Engine)\n",
      "p = ps[2]\n",
      "print \"Related posts for \" + p.title\n",
      "for id in searcher(p):\n",
      "    print \"   -\", DBSession.query(Post).get(id).title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Related posts for OSPF Convergence Concept - Adv Router not-reachable\n",
        "   -"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Trying to understand problems that occur when redistributing between two OSPF processes\n",
        "   - 2811 LAN to LAN IPSEC\n",
        "   - Different advertisement of redistributed static routes in OSPF on the same router due to OSPF broadcast network type?\n",
        "   - OSPF Peers\n",
        "   - Help with OSPF and DMVPN\n",
        "Wall time: 9.95 s\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Not perfect, but not too bad either, and probably some more manual validation and tuning could make the approach really good. Being pressed for time, we won't dig into fine-tuning and proceed with the alternatives."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3.1.2 Simhash-based similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Simhash is a very simple locality-sensitive-hashing algorithm for texts that is realy good at finding near-duplicate texts in collections (see the appropriate paper for description). My implementation of simhash is available in `tx.similarity.SimHashIndexer`. The algorithm is modified to only consider \"informative\" words that we extracted in the previous section. Let us try using it to hash the full set of documents and see how the responses work."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "word_sets = map(lambda p: tx.features.SET_OF_WORDS(p.all_text), ps) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 26min 22s\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "simhashes = map(tx.features.SIMHASH, word_sets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 3min 38s\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "ix_sh = tx.similarity.SimHashIndexer()\n",
      "for h, p in zip(simhashes, ps):\n",
      "    ix_sh.add_hash(h, p.id)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 671 ms\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "p = ps[2]\n",
      "print p.title\n",
      "for i in ix_sh.find(p.all_text):\n",
      "    if i != p.id:\n",
      "        print \"  -\", DBSession.query(Post).get(i).title"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OSPF Convergence Concept - Adv Router not-reachable\n",
        "  - BGP Mulisite Multihoming\n",
        "  - No access FTP browser (remotely)\n",
        "  - Wifi networok issue on cisco router\n",
        "  - HELP ON FRAME-RELAY\n",
        "Wall time: 31 ms\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we see, the speed is lightning-fast, but there are less matches and the revance is smaller. Indeed, SimHash usually works best when tuned for identifying near-duplicates, otherwise the results may be somewhat random. BTW, it turns out there are no near duplicates in the data. Oh, well. OK. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3.1.3 TextWinnowing-based similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The TextWinnowing algorithm is capable of locating short (essentially k-word) similar pieces of text in the document. As such it is more suitable for plagiarism detection rather than suggestions and won't necessarily work well. However I have a spare personal implementation lying and I am curious to try out."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "ix_tw = tx.similarity.TextWinnowingIndexer()\n",
      "hashes = [ix_tw.hasher(p.all_text) for p in ps]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 28min 58s\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ix_tw = tx.similarity.TextWinnowingIndexer()\n",
      "for h, p in zip(hashes, ps):\n",
      "    ix_tw.add_hash(h, p.id)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "p = ps[5]\n",
      "print p.title\n",
      "for i in ix_tw.find(p.all_text):\n",
      "    if i != p.id:\n",
      "        print \"  -\", DBSession.query(Post).get(i).title"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dhcp snooping vs. giaddr\n",
        "  - IP Source Guard &amp; WiFi roaming\n",
        "  - how to find out other dhcp server in my network?\n",
        "  - Relation between DHCP snooping and DHCP trust\n",
        "  - DHCP snooping vs routed port\n",
        "  - DAI/IPSG question\n",
        "  - Invalid ARP\n",
        "  - DHCP snooping : dropped packets\n",
        "  - DHCP Snooping, DAI, IP source guard\n",
        "  - DHCP snooping not working on 2960s ver 15.0(1)SE3\n",
        "  - DHCP snooping design\n",
        "Wall time: 16 ms\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Subjectively, the quality of results here is better than for SimHash, but less diverse than FullText. The speed is great."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3.1.4 Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To summarize, I propose to deploy a similarity recommendation engine for discussions. Three prototype implementations are presented above, of which two might be quite decent for practical purposes with just a bit of tuning."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "3.2 Automated tagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next let us try to design and test a simple automated tagging engine. In order to train the engine we need labeled data. The data we have lacks tags, so rather than using Cisco forums, we shall train a tag recognizer on the StackOverflow data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The tagging model we shall consider will be based on simple weighted word features. We first extract up to 200 most informative words with respect to each tag using a simple binomial p-value test. We will then use the extracted features to train a simple classifier (picking the best among a linear, naive Bayes or a small decision tree). The resulting model can be serialized and applied on Cisco data.\n",
      "\n",
      "The detailed transcript of those experiments is extracted to a [separate notebook](5 - Automated tagging.ipynb). (Note that there's the big data, the cluster computing, the statistics, and the machine learning all in one, so check it out)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we load the already trained models and test how they work with the Cisco data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "with open('data/tagging.pickle') as f:\n",
      "    models = pickle.load(f)\n",
      "ms = [m[0] for m in models]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "fe = lambda x: Counter(x)\n",
      "\n",
      "# This function takes a word-set extracted from the fulltext using our feature extractor and predicts tags\n",
      "# (We have already computed all the word sets so we'll avoid recomputing them here)\n",
      "def predict_tags(word_set):\n",
      "    return [m.tag_name for m in ms if m.predict_worddict(fe(word_set)) is not None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try adding tags to existing posts, and see what we get from there.\n",
      "(Note: this only uses 10k documents, as I did not have the time to re-run it for the full dataset)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "tag_predictions = defaultdict(list)\n",
      "for i, ws in enumerate(word_sets):\n",
      "    ts = predict_tags(ws)\n",
      "    for tag in ts:\n",
      "        tag_predictions[tag].append(i)\n",
      "    if (i % 1000 == 0):\n",
      "        print '.',"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k,v in tag_predictions.iteritems():\n",
      "    print 'Tag: %s assigned to %d posts' % (k, len(v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tag: wildcards assigned to 1 posts\n",
        "Tag: linux-kernel assigned to 40 posts\n",
        "Tag: solaris assigned to 1 posts\n",
        "Tag: xorg assigned to 2 posts\n",
        "Tag: tar assigned to 16 posts\n",
        "Tag: kernel assigned to 4 posts\n",
        "Tag: process assigned to 4 posts\n",
        "Tag: ip assigned to 1 posts\n",
        "Tag: keyboard assigned to 4 posts\n",
        "Tag: apt assigned to 5 posts\n",
        "Tag: cron assigned to 2 posts\n",
        "Tag: linux assigned to 1 posts\n",
        "Tag: text-processing assigned to 3 posts\n",
        "Tag: find assigned to 2 posts\n",
        "Tag: virtualbox assigned to 1 posts\n",
        "Tag: logs assigned to 5 posts\n",
        "Tag: password assigned to 36 posts\n",
        "Tag: samba assigned to 12 posts\n",
        "Tag: networking assigned to 397 posts\n",
        "Tag: command-line assigned to 3 posts\n",
        "Tag: iptables assigned to 5 posts\n",
        "Tag: boot assigned to 1 posts\n",
        "Tag: perl assigned to 7 posts\n",
        "Tag: terminal assigned to 10 posts\n",
        "Tag: compiling assigned to 2 posts\n",
        "Tag: ls assigned to 3 posts\n",
        "Tag: dns assigned to 5 posts\n",
        "Tag: memory assigned to 1 posts\n",
        "Tag: performance assigned to 1 posts\n",
        "Tag: email assigned to 5 posts\n",
        "Tag: rhel assigned to 5 posts\n",
        "Tag: files assigned to 41 posts\n",
        "Tag: shell assigned to 3 posts\n",
        "Tag: package-management assigned to 6 posts\n",
        "Tag: screen assigned to 2 posts\n",
        "Tag: symlink assigned to 1 posts\n",
        "Tag: opensuse assigned to 1 posts\n",
        "Tag: ssh assigned to 51 posts\n",
        "Tag: hard-disk assigned to 6 posts\n",
        "Tag: ubuntu assigned to 7 posts\n",
        "Tag: apache assigned to 1 posts\n",
        "Tag: php assigned to 2 posts\n",
        "Tag: debian assigned to 2 posts\n",
        "Tag: permissions assigned to 9 posts\n",
        "Tag: io-redirection assigned to 3 posts\n",
        "Tag: grep assigned to 3 posts\n",
        "Tag: gnome assigned to 2 posts\n",
        "Tag: windows assigned to 9 posts\n",
        "Tag: root assigned to 1 posts\n",
        "Tag: mount assigned to 5 posts\n",
        "Tag: wifi assigned to 5 posts\n",
        "Tag: software-installation assigned to 10 posts\n",
        "Tag: freebsd assigned to 3 posts\n",
        "Tag: osx assigned to 3 posts\n",
        "Tag: directory assigned to 7 posts\n",
        "Tag: x11 assigned to 3 posts\n",
        "Tag: security assigned to 11 posts\n",
        "Tag: audio assigned to 3 posts\n",
        "Tag: backup assigned to 5 posts\n",
        "Tag: scripting assigned to 37 posts\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do the tag make sense? It seems that sometimes they do:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in tag_predictions['memory']:\n",
      "    print ps[i].title,'\\t', ps[i].url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ciscoworks directory size is increasing rapidly afte installation. \thttps://supportforums.cisco.com/discussion/10700636/ciscoworks-directory-size-increasing-rapidly-afte-installation\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in tag_predictions['security']:\n",
      "    print ps[i].title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1721 Config Update, Pinholes and Help \t\n",
        "2620XM didn't find WIC 4ESW \t\n",
        "PAP authentication in PPP \t\n",
        "ICMP unreachable, rate-limit command \t\n",
        "A few questions about router security, premise routers, BGP \t\n",
        "Can you copy a Catalyst 3500XL existing IOS software? \t\n",
        "Cisco SCE1000/2000 not recognised by CiscoWorks \t\n",
        "hardening access layer switches / security \t\n",
        "Setting MTU on FE interfaces. Lack of documentation? \t\n",
        "Wan Interface card doesnt get recognised \t\n",
        "Installing extra DRAM causes problems with flash \t\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in tag_predictions['performance']:\n",
      "    print ps[i].title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Oracle JDBC and performance \t\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And for most \"non-cisco\" terms they probably don't:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in tag_predictions['windows']:\n",
      "    print ps[i].title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Modem dial in to C878 console port \t\n",
        "LMS 2.6 SWIM Job Stuck \t\n",
        "Mobile Broadband Design \t\n",
        "Two Internet leased line config at the same router? \t\n",
        "Packet capture strange issue ! \t\n",
        "2 Internet connections + IpSec VPN + site to site vpn \t\n",
        "Remote office Connection \t\n",
        "8 WAN bonding - is it possible by cisco? \t\n",
        "PPPoE in Cisco 1841 router with dynamic ip address \t\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in tag_predictions['audio']:\n",
      "    print ps[i].title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "spanning tree between cisco and nortel switches \t\n",
        "How to replace analog lease line with another cost effective medium \t\n",
        "Forwarding Multicast over WAN \t\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in tag_predictions['package-management']:\n",
      "    print ps[i].title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "upgrade path and cost \t\n",
        "LMS upgrade path and cost \t\n",
        "LMS 2.6 SWIM Job Stuck \t\n",
        "QoS - Voice Traffic Queueing \t\n",
        "LMS 2.6:Inventory update failing \t\n",
        "LMS 2.6: Change the Domain membership \t\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3.3 Audience structure analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, I've completed the preliminary runs of the above tasks on a 10k sample. I'll relaunch them on the full dataset and while they run here, I shall use [a separate notebook](6 - Audience structure analysis.ipynb) to play with descriptive analysis. I am interested to validate the assumption that the users of the forum indeed consist of \"askers\" and \"answerers\"."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}