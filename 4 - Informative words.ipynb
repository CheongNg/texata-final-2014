{
 "metadata": {
  "name": "",
  "signature": "sha256:61d6bafa3b34610b28daf692dfb83a928a96449273daab6ef5ae34102b3240c9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Computing a list of informative words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The hard part here is counting the words in the data. To be more precise, there is nothing hard about it, but I am using a great Python package for word stemming and lemmatization `TextBlob`, which can be somewhat slow at times. Hence, to avoid the wait I'll run the analysis using IPython's parallelization tools.\n",
      "\n",
      "Update: as Cisco prohibited transferring the data for analysis to external computers, I could not run this extraction procedure for full Cisco forum data and used words extracted previously from SO data instead. The code below is an experiment run on the sample data provided before the contest. The actual `informative.words.txt` file is a result of a different extraction."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start by reading in all the posts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from tx.db import *\n",
      "db_url = 'postgresql://texata:__austin__@localhost:5432/texata_test'\n",
      "connect_db(db_url)\n",
      "ps = DBSession.query(Post).all()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 3.61 s\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from tx.features import *\n",
      "from collections import Counter\n",
      "fe = compose(Counter, SetOfWords())\n",
      "wcs = map(lambda p: fe(p.all_text), ps)\n",
      "\n",
      "def join_bags(x, y):\n",
      "    x.update(y)\n",
      "    return x\n",
      "wcs = reduce(join_bags, wcs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 9.71 s\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wcs = wcs.most_common()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we've got for each word the number of posts it is present in:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wcs[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[(u'nbsp', 6), (u'0', 6), (u'cisco', 5), (u'wa', 5), (u'1', 5)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Leave only the words that are present at least 3 times."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_words = [word for (word, count) in wcs if count >= 3]\n",
      "len(good_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "16"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Drop from this list the words that are considered \"common\" (the Oxford3000 list) and the stopwords from nltk."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import Word\n",
      "ox_words = [str(Word(w.strip()).lemmatize()) for w in open('data/oxford3000.wordlist.txt')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "stop_words = nltk.corpus.stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package stopwords to\n",
        "[nltk_data]     C:\\Users\\Konstantin\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package stopwords is already up-to-date!\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "informative_words = set(good_words).difference(ox_words).difference(stop_words)\n",
      "len(informative_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "16"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('informative.wordlist.txt', 'w') as f:\n",
      "    for w in interesting_words:\n",
      "        f.write(w.encode('utf-8') + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ".. and we're done."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}