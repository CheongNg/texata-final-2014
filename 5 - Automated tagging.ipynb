{
 "metadata": {
  "name": "",
  "signature": "sha256:d80688975d5f66e65818ecb0f91bedc3d87ebfa00a059b1c00d8c5378bf86ce2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Deriving an automated tagging model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To derive the automated tagging model, I shall use the data from the stackoverflow's `unix`, `serverfault`, `security` and `networkengineering` subsites. I won't take the full SO as downloading it would take too much time, but the approach below should scale to the full SO as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# Download the \"Post\" and \"Tag\" data from all four databases\n",
      "from tx.db_so import *\n",
      "from collections import Counter\n",
      "\n",
      "db_url = 'postgresql://ubuntu:__austin__@master:5432/%s'\n",
      "all_posts = []\n",
      "all_tags = Counter()\n",
      "\n",
      "for db in ['unix', 'serverfault', 'security', 'networkengineering']:\n",
      "    connect_db(db_url % 'unix')\n",
      "    ps = DBSession.query(Post).filter(Post.post_type_id == 1).all()\n",
      "    all_posts.extend(ps)\n",
      "    tags = DBSession.query(Tag.tag_name, Tag.count).all()\n",
      "    all_tags.update(dict(tags))    \n",
      "    DBSession.remove()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 9.91 s, sys: 2.15 s, total: 12.1 s\n",
        "Wall time: 13.4 s\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(all_posts), len(all_tags)\n",
      "print all_tags.most_common(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "193816 1943\n",
        "[(u'linux', 31148), (u'bash', 17404), (u'shell', 11300), (u'debian', 11232), (u'ubuntu', 10288)]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we convert each post to a set of words representation. To speed this up we'll use IPython's parallelization capabilities:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "c = Client()\n",
      "print \"Worker count: %d\" % len(c.ids)\n",
      "cluster = c[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Worker count: 35\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "%px import tx\n",
      "%px from collections import Counter\n",
      "%px fe = tx.features.compose(Counter, tx.features.SetOfWords())\n",
      "word_sets = cluster.map(lambda p: fe(p.all_text), all_posts, block=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 53.9 s, sys: 5.42 s, total: 59.3 s\n",
        "Wall time: 1min 12s\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us now first try to predict the tag `linux` (the most frequent one so far)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_name = '<linux>'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from tx.features import nvl\n",
      "label = [tag_name in nvl(p.tags) for p in all_posts]\n",
      "print \"Posts with tag: %d, Posts total %d\" % (sum(label), len(label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Posts with tag: 31148, Posts total 193816\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Feature extraction: find significant discriminative words and use them as binary features of a simple classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from collections import Counter\n",
      "total_word_counts = Counter()\n",
      "tag_word_counts = Counter()\n",
      "for i, wc in enumerate(word_sets):\n",
      "    total_word_counts.update(wc)\n",
      "    if label[i]:\n",
      "        tag_word_counts.update(wc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 4.8 s, sys: 0 ns, total: 4.8 s\n",
        "Wall time: 4.74 s\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "import scipy\n",
      "pvs = {w: scipy.stats.binom_test(tag_word_counts[w], sum(label), float(total_word_counts[w])/len(label))\n",
      "       for w in tag_word_counts}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Turns out the exact binomial test is too slow, we'll do a gaussian z-score approximation instead then."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zscores = dict()\n",
      "n = sum(label)\n",
      "for w in tag_word_counts:\n",
      "    p = float(total_word_counts[w])/len(label)\n",
      "    zscores[w] = abs((tag_word_counts[w] - n*p)/sqrt(n*p*(1-p)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Select at most 200 most discriminative words\n",
      "word_ranking = sorted([(-v, k) for k, v in zscores.iteritems()])\n",
      "word_ranking[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "[(-95.465742017397517, u'linux'),\n",
        " (-56.721830596013184, u'kernel'),\n",
        " (-26.802541218087246, u'pre'),\n",
        " (-25.629846992109513, u'bash'),\n",
        " (-22.835880901540996, u'ram'),\n",
        " (-22.145071102259067, u'proc'),\n",
        " (-21.821843612555327, u'distro'),\n",
        " (-20.818629852317343, u'embedded'),\n",
        " (-19.528605580340891, u'hardware'),\n",
        " (-18.975553020581717, u'cpu')]"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len([w for v, w in word_ranking if v < -10]) # Fair enough"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "113"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ideally, we would prefer to experiment a bit in choosing the number of words, but for demonstration purposes we'll leave it at that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = sorted([w for v, w in word_ranking if v < -10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# Now extract features from each document\n",
      "%px import numpy as np\n",
      "cluster['features'] = features\n",
      "def extract_features(ws):\n",
      "    return np.array([ws[w] for w in features])\n",
      "X = cluster.map(extract_features, word_sets, block=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1min, sys: 619 ms, total: 1min 1s\n",
        "Wall time: 1min 1s\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll do the training and testing. As the time is limited, we'll avoid fine-tuning and just crudely check which of the basic classifiers works best on the training set. We avoid holdout testing and only pick simple models that most probably won't overfit anyway."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "clfs = [BernoulliNB(), MultinomialNB(), LinearSVC(), DecisionTreeClassifier(max_depth=4), LogisticRegression()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for clf in clfs:\n",
      "    %time clf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1.88 s, sys: 584 ms, total: 2.46 s\n",
        "Wall time: 2.46 s\n",
        "CPU times: user 1.68 s, sys: 401 ms, total: 2.08 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 2.08 s\n",
        "CPU times: user 29.8 s, sys: 280 ms, total: 30.1 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 30.1 s\n",
        "CPU times: user 7.86 s, sys: 68.1 ms, total: 7.93 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 7.93 s\n",
        "CPU times: user 5.06 s, sys: 346 ms, total: 5.41 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 5.41 s\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
      "for clf in clfs:\n",
      "    yhat = clf.predict(X)\n",
      "    print clf.score(X, y), precision_score(y, yhat), recall_score(y, yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.814917241095 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.376541919298 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.23128290741\n",
        "0.826288851282"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.397260273973 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.156414537049\n",
        "0.841788087671"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.619801980198 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0401951971234\n",
        "0.840384694762"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.72268907563 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0110440477719\n",
        "0.840983200561"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.550617283951 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0572749454219\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The results are fairly similar with all kinds of tradeoffs (NB is fast+imprecise, SVM is slow+precise, DecisionTree is average speed+bad recall, Logistic regression somewhere inbetween). As training is a one-off procedure here, I think SVM seems like a reasonable choice. \n",
      "\n",
      "Now let us examine some false positives to get a feeling at whether the classifier tags things wrongly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = clfs[2]\n",
      "yhat = clf.predict(X)\n",
      "false_positives = find(logical_and(yhat, logical_not(y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_posts[false_positives[0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "Post\n",
        "\tid: 1235\n",
        "\taccepted_answer_id: None\n",
        "\tanswer_count: 1\n",
        "\tcomment_count: 0\n",
        "\tcreation_date: 2010-08-25 04:48:04.753000\n",
        "\tfavorite_count: 1\n",
        "\tlast_activity_date: 2014-07-26 11:09:28.210000\n",
        "\tlast_edit_date: 2010-11-07 21:55:36.320000\n",
        "\towner_user_id: 1205\n",
        "\tparent_id: None\n",
        "\tpost_type_id: 1\n",
        "\tscore: 2\n",
        "\ttags: <software-rec><data-recovery><iphone>\n",
        "\tview_count: 2186\n",
        "\ttitle: Accessing the iPhone file system via Linux/Unix\n",
        "\tbody: <p><a href=\"http://gadgets.stackexchange.com/questions/394/any-way-to-recover-data-from-a-semi-bricked-iphone\">My old iPhone is semi-bricked</a>. It turns off after a few minutes and doesn't get past the load screen.</p>\n",
        "\n",
        "<p>It has important data on it which wasn't backed up. I'm trying to access the hard-drive's contents without resorting to a hard-drive recovery company.</p>\n",
        "\n",
        "<p>Someone mentioned to me that you can <a href=\"https://help.ubuntu.com/community/PortableDevices/iPhone\" rel=\"nofollow\">use a Linux machine and the program Amarok</a> to access the drive's contents. However, my phone wasn't jailbroken nor had the usb tethering option enabled. Plus, it had a passcode on it, so I'm guessing this method wouldn't work.</p>\n",
        "\n",
        "<p>Are there any other ways you can think of to access the drives contents using a Unix machine? Is there any way to take it apart and connect a cable directly to the hard-drive?</p>\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we see, the false positives are not false positives, but actually a decent suggestions for adding the `<linux>` tag. Hence we might conclude that the tagging algorithm is OK."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us try with another tag (and with some code stashed away in the package `tx.tagging`)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from tx.tagging import *\n",
      "import random\n",
      "random.seed(1)\n",
      "posts = random.sample(all_posts, 5000)\n",
      "tm, acc, prec, rec = train_tag_model(posts, 'bash')\n",
      "print \"Accuracy %f, Precision %f, Recall %f\" % (acc, prec, rec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy 0.937000, Precision 0.669333, Recall 0.567873\n",
        "CPU times: user 19.9 s, sys: 51 ms, total: 19.9 s\n",
        "Wall time: 19.9 s\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks rather cool. Now we shall run it for the top 100 tags."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(all_posts)\n",
      "cluster.scatter('all_posts', all_posts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_tags = [tag_name for tag_name, count in all_tags.most_common(100)]\n",
      "cluster.scatter('top_tags', top_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "<AsyncResult: scatter>"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "%%px \n",
      "from tx.tagging import train_tag_model\n",
      "ms = [train_tag_model(all_posts, tag) for tag in top_tags]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 16.9 s, sys: 981 ms, total: 17.8 s\n",
        "Wall time: 2min 18s\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ms = cluster.gather('ms', block=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# .. We'll save the models as a single pickle\n",
      "import pickle\n",
      "with open('data/tagging.pickle', 'w') as f:\n",
      "    pickle.dump(ms, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    }
   ],
   "metadata": {}
  }
 ]
}